{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/1NmbuVh/0t5SGzaZmfEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathwikmethari/diffusers_ex_collab/blob/main/lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "pipe=DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",torch_dtype=torch.float16).to(device)"
      ],
      "metadata": {
        "id": "hOP9Zey37BjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load_lora_weights**\n",
        "\n",
        "( pretrained_model_name_or_path_or_dict: typing.Union[str, typing.Dict[str, torch.Tensor]], adapter_name: typing.Optional[str] = None**kwargs )\n",
        "\n",
        "Parameters::\n",
        "\n",
        "*pretrained_model_name_or_path_or_dict (str or os.PathLike or dict)* — See lora_state_dict().\n",
        "\n",
        "*adapter_name (str, optional)* — Adapter name to be used for referencing the loaded adapter model. If not specified, it will use default_{i} where i is the total number of adapters being loaded.\n",
        "\n",
        "*low_cpu_mem_usage (bool, optional)* — Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.\n",
        "\n",
        "*kwargs (dict, optional)* — See lora_state_dict().\n",
        "\n",
        "Load LoRA weights specified in pretrained_model_name_or_path_or_dict into self.unet and self.text_encoder.\n",
        "\n",
        "All kwargs are forwarded to self.lora_state_dict.\n",
        "See lora_state_dict() for more details on how the state dict is loaded.\n",
        "See load_lora_into_unet() for more details on how the state dict is loaded into self.unet.\n",
        "See load_lora_into_text_encoder() for more details on how the state dict is loaded into self.text_encoder."
      ],
      "metadata": {
        "id": "tQ-Yj1xV9NJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")"
      ],
      "metadata": {
        "id": "3EG4YnG_8ePh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"toy_face of a hacker with a hoodie\"\n",
        "\n",
        "lora_scale = 0.9\n",
        "image = pipe(\n",
        "    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n",
        ").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "JeThB33v8OSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use multiple loras using *set_adapters()*\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "*adapter_names (List[str] or str)* — The names of the adapters to use.\n",
        "\n",
        "*adapter_weights (Union[List[float], float], optional)* — The adapter(s) weights to use with the UNet. If None, the weights are set to 1.0 for all the adapters.\n",
        "\n",
        "can disable it using *disable_lora()*\n"
      ],
      "metadata": {
        "id": "QfqIQRXEDfK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n",
        "# pipe.set_adapters([\"toy\", \"pixel\"], adapter_weights=[0.5, 0.3])\n",
        "\n",
        "prompt = \"toy_face of a pixelated hacker with a hoodie\"\n",
        "\n",
        "lora_scale = 1\n",
        "image = pipe(\n",
        "    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n",
        ").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "BlGy3ToLBXpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customize adapters strength**\n",
        "\n",
        "For even more customization, you can control how strongly the adapter affects each part of the pipeline. For this, pass a dictionary with the control strengths (called “scales”) to set_adapters().\n",
        "\n",
        "For example, here's how you can turn on the adapter for the down parts, but turn it off for the mid and up parts:"
      ],
      "metadata": {
        "id": "NMo0TyBoErXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.enable_lora()  # enable lora again, after we disabled it above\n",
        "prompt = \"toy_face of a hacker with a hoodie, pixel art\"\n",
        "adapter_weight_scales = { \"unet\": { \"down\": 1, \"mid\": 0, \"up\": 0} }\n",
        "pipe.set_adapters(\"pixel\", adapter_weight_scales)\n",
        "image = pipe(prompt, num_inference_steps=30, generator=torch.manual_seed(0)).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "VvpS41-xEf3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.config"
      ],
      "metadata": {
        "id": "C6JKn4jDFS1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manage adapters**\n",
        "\n",
        "You have attached multiple adapters in this tutorial, and if you’re feeling a bit lost on what adapters have been attached to the pipeline’s components, use the *get_active_adapters()* method to check the list of active adapters:"
      ],
      "metadata": {
        "id": "xNo-AGOCFmes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "active_adapters = pipe.get_active_adapters()\n",
        "active_adapters"
      ],
      "metadata": {
        "id": "_jqPThVdFUDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can also get the active adapters of each pipeline component with get_list_adapters():\n",
        "list_adapters_component_wise = pipe.get_list_adapters()\n",
        "list_adapters_component_wise"
      ],
      "metadata": {
        "id": "-P-vv53IF2tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4-mJVzQGDbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}